{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# **IA**\n",
    "## **Explication de l'algorithme Adaboost**\n"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Introduction : les algorithmes de boosting"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Le boosting est une technique de modélisation d'ensemble qui a été présentée pour la première fois par Freund et Schapire en 1997.<br>\n",
    "Depuis lors, le boosting est une technique courante pour résoudre les problèmes de classification binaire.<br>\n",
    "Ces algorithmes améliorent le pouvoir de prédiction en convertissant un certain nombre d'apprenants faibles en apprenants forts.<br>\n",
    "<br>\n",
    "Le principe des algorithmes de boosting est le suivant : nous construisons d'abord un modèle sur l'ensemble des données d'apprentissage, puis un deuxième modèle est construit pour rectifier les erreurs présentes dans le premier modèle.<br>\n",
    "Cette procédure est poursuivie jusqu'à ce que les erreurs soient minimisées et que le jeu de données soit correctement prédit.<br>\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fonctionnement de l'agorithme AdaBoost"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "AdaBoost, également appelé Adaptive Boosting, est une technique d'apprentissage automatique utilisée comme méthode d'ensemble. <br>\n",
    "Il construit un modèle et donne une pondération égale à tous les points de données.<br>\n",
    "Il attribue ensuite des pondérations plus élevées aux points qui sont mal classés. Maintenant, tous les points qui ont un poids plus élevé ont plus d'importance dans le prochain modèle.<br>\n",
    "Il continuera à former des modèles jusqu'à ce qu'une erreur faible soit reçue.<br>\n",
    "\n",
    "L'algorithme le plus communément utilisé avec AdaBoost est celui des arbres de décision à un niveau, c'est-à-dire avec des arbres de décision à une seule division.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Avantages et inconvénients"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "AdaBoost est notablement sensible aux données bruitées ou peu corrélées.<br>\n",
    "Toutefois, dans certains problèmes, il peut s'avérer moins enclin au surapprentissage que d'autres algorithmes.<br>\n",
    "Les sous-classeurs utilisés peuvent être faibles tant qu'ils proposent une performance au moins un peu supérieure à celle d'un classeur aléatoire, auquel cas il peut être prouvé que le modèle final converge vers un classeur fort."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Implémentation dans Python"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dans Python, il est possible de créer des modèles AdaBoost grâce à la classe **AdaboostClassifier** du module **ensemble** de **sklearn** .\n",
    "Les hyperparamètres à configurer essentiellement sont :\n",
    "\n",
    "- base_estimator : Le modèle de l'ensemble, le défaut est un arbre de décision.\n",
    "\n",
    "- n_estimatorsrs : Le nombre de modèles à construire.\n",
    "\n",
    "- learning_rate : Réduit la contribution de chaque classificateur par cette valeur.\n",
    "\n",
    "- random_state : L'echantillion de nombres aléatoires, afin que les mêmes nombres aléatoires soient générés à chaque fois."
   ],
   "metadata": {}
  }
 ]
}
